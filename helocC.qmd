---
title: "HELOC Recourse - Algorithm 3"
format: html
editor: visual
theme: cosmo
---

```{=tex}
\newcommand{\pa}{\mathrm{pa}}
\newcommand{\Pa}{\mathrm{Pa}}
```
```{r setup}
#| echo: false
#| message: false
set.seed(2023)
root <- rprojroot::find_root(rprojroot::is_git_root)
r_dir <- file.path(root, "r")
invisible(lapply(list.files(r_dir, full.names = TRUE), source))
data <- read.csv(file.path(root, "data/heloc_dataset_v1.csv"))

top6 <- c("AverageMInFile", "PercentTradesWBalance",
          "PercentTradesNeverDelq", "MSinceMostRecentInqexcl7days",
          "NetFractionRevolvingBurden", "ExternalRiskEstimate")

out <- "RiskPerformance"

n <- 10^4
all_dat <- heloc_synth(nsynth = n, frank = FALSE, mc_samp = 100)
data <- all_dat[["data"]][seq_len(n), ]
data <- data[complete.cases(data), ]
synth_data <- all_dat[["synth_data"]][seq_len(n), ]
rec_data <- all_dat[["rec_data"]][seq_len(n), ]
rec_mc <- all_dat[["rec_mc"]]
```

## Inspecting the data

```{r}
knitr::kable(head(synth_data), caption = "HELOC Synthetic dataset.")
```

## Constructing the Causal Diagram

```{r dag}
#| fig-align: center
adj.mat <- array(0, dim = c(length(c(top6, out)), length(c(top6, out))))
colnames(adj.mat) <- rownames(adj.mat) <- c(top6, out)

grp <- list(
  g1 = c("AverageMInFile"),
  g3 = c("PercentTradesWBalance", "PercentTradesNeverDelq"),
  g4 = c("MSinceMostRecentInqexcl7days", "NetFractionRevolvingBurden"),
  g5 = c("ExternalRiskEstimate"),
  g6 = out
)

coord <- matrix(c(0, 0), ncol = 2L)

for (i in seq.int(2, length(grp))) {
  
  adj.mat[unlist(grp[seq.int(1, i-1)]), grp[[i]]] <- 1
  coord <- rbind(coord, cbind(c(seq_along(grp[[i]]) - mean(seq_along(grp[[i]]))), -(i-1)))
}

# sort topologically
top.ord <- topologicalOrdering(adj.mat)
adj.mat <- adj.mat[top.ord, top.ord]

par(mar=c(0,0,0,0)+.1)
plot(fairadapt::graphModel(adj.mat), layout = coord, vertex.size=20, 
     vertex.label.cex=0.8)
```

## Train an `xgboost` predictor on real data

```{r xgb}
xgbcv <- xgb.cv(params = list(eta = 0.1), data = as.matrix(data[, top6]), 
                label = data[[out]], nrounds = 100, 
                early_stopping_rounds = 3,
                objective = "binary:logistic",
                nfold = 10, verbose = FALSE)

nrounds <- xgbcv$best_iteration
xgb <- xgboost(params = list(eta = 0.1), data = as.matrix(data[, top6]), 
               label = data[[out]], nrounds = nrounds, verbose = FALSE)
```

## Rejection of $H_0$

```{r recourse}
# define recourse action do(C3 = 85, D2 = 20)
rec_act <- list(
  list(
    PercentTradesNeverDelq = 0.85,
    NetFractionRevolvingBurden = 0.2
  )
)

# attempt to fit a copula model and infer the tau parameter 
result <- tryCatch({
  
  course_obj <- fairecourse(
    ~ ., adj.mat = adj.mat[-7, -7], train.data = synth_data, 
    rec_act = rec_act, pre.rec = tail(synth_data, n = 2000), 
    post.rec = tail(rec_data, n = 2000)
  )
}, error = function(e) {

  cat(e$message)
})
```

Since the ground truth conditional quantile distribution is given by \begin{align}
  C(q_2 \mid q_1) = \text{Unif}[q_1, 1],
\end{align} the $H_0$ hypothesis of a Frank copula quantile coupling is rejected.

## Train With Copula

Nonethless, we fit a copula-based model as a baseline. This can be achieved by passing the argument `soft.test = TRUE` which ensures that the procedure does not result in an error (but a message) even if the $H_0$ hypothesis is rejected.

```{r with-copula}
cop_with <- fairecourse(
    ~ ., adj.mat = adj.mat[-7, -7], train.data = synth_data, 
    rec_act = rec_act, pre.rec = synth_data, 
    post.rec = rec_data, soft.test = TRUE
)
```

## Copula-free Learning

Finally, we fit a model that performs copula-free learning from Alg. 3, by regressing \begin{align}
  V^*_i \sim \mathrm{Pa}^*_i + V_i + \mathrm{Pa}_i.
\end{align}

```{r copula-free}
cop_free <- fairecourse(
  ~ ., adj.mat = adj.mat[-7, -7], train.data = NULL, 
  rec_act = rec_act, pre.rec = synth_data, 
  post.rec = rec_data
)
```

As another baseline model, we also consider the model of using only the post-recourse parents and recourse data, in which we regress \begin{align}
  V^*_i \sim \mathrm{Pa}^*_i.
\end{align}

## $\widehat{Y}(u^*)$ Spread Coverage

For $n_{\text{test}} = 50$ samples, we compute 100 Monte Carlo samples using (i) copula-free learning from Alg. 3 (labeled $\widehat{Y}_{R=r}(u^*)$); (ii) copula-based learning using Alg. 1 ($\widehat{Y}^F_{R=r}(u^*)$); (iii) recourse data learning based on the model $V^*_i \sim \mathrm{Pa}^*_i$ ($\widehat{Y}^\text{exp}_{R=r}(u^*)$); (iv) samples from the true underlying distribution based on the SCM ($\widehat{Y}^\text{true}_{R=r}(u^*)$).

```{r yhat-spread-coverage}
n_tst <- 50

rec_free <- predict(cop_free, data = tail(synth_data, n = n_tst), 
                    rec_act = rec_act, mc_samp = 100)

yhat_free <- predict(
  xgb, newdata = as.matrix(rec_free[, top6])
)

rec_cp <- predict(cop_with, data = tail(synth_data, n = n_tst), 
                  rec_act = rec_act, mc_samp = 100)

yhat_cp <- predict(
  xgb, as.matrix(rec_cp[, top6])
)

rec_exp <- predict(cop_free, data = tail(synth_data, n = n_tst), 
                   rec_act = rec_act, mc_samp = 100, use_obs = FALSE)

yhat_exp <- predict(
  xgb, newdata = as.matrix(rec_exp[, top6])
)

# obtain predictions for the generated ground truth
yhat_true <- predict(
  xgb, as.matrix(rec_mc[, top6])
)
```

We inspect the spread of the MC samples for a randomly selected individual.

```{r plot-spread-indiv-1}
#| echo: false
#| fig-align: center
idx1 <- rec_free$indiv == 1
spread1 <- rbind(
  data.frame(x = yhat_free[idx1], Estimation = "Copula-Free Estimation"),
  data.frame(x = yhat_cp[idx1], Estimation = "Copula-Based Estimation"),
  data.frame(x = yhat_exp[idx1], Estimation = "Exp"),
  data.frame(x = yhat_true[idx1], Estimation = "Underlying Truth")
)

ggplot(spread1, aes(x = x, fill = Estimation)) +
  geom_density(alpha = 0.4) + theme_minimal() +
  xlab(latex2exp::TeX("$\\widehat{Y}$ distribution")) +
  ylab("Density") +
  scale_fill_discrete(
    labels = c("Copula-Based", "Copula-Free",
               latex2exp::TeX("$V^*_i | pa^*_i$"), "Ground Truth")
  ) +
  theme(legend.position = "bottom")
```

## Kolmogorov-Smirnov Distances from Ground Truth

Finally, we compute the Kolmogorov-Smirnov (KS) statistics for the distance of the predicted MC empirical distributions to the underlying true distribution. In particular, we compute \begin{align}
  D_n&(\widehat{Y}_{R=r}(u^*), \widehat{Y}^\text{true}_{R=r}(u^*)) \\
  D_n&(\widehat{Y}^F_{R=r}(u^*), \widehat{Y}^\text{true}_{R=r}(u^*)) \\
  D_n&(\widehat{Y}^\text{exp}_{R=r}(u^*), \widehat{Y}^\text{true}_{R=r}(u^*))
\end{align} where $D_n(A, B)$ is the KS statistic computed as $\sup_{x \in \mathbb{R}} |F_A(x) - F_B(x)|$ where $F_A, F_B$ are empirical cumulative distribution functions of samples $A, B$.

```{r ks-stats, warning=FALSE}
ks_frame <- NULL
for (i in seq_len(n_tst)) {
  
  idxi <- rec_free$indiv == i
  ks_frame <- rbind(
    ks_frame,
    data.frame(ks = ks.test(x = yhat_free[idxi], yhat_true[idxi])$statistic,
               Distribution = "Copula-Free"),
    data.frame(ks = ks.test(x = yhat_cp[idxi], yhat_true[idxi])$statistic,
               Distribution = "Copula-Based"),
    data.frame(ks = ks.test(x = yhat_exp[idxi], yhat_true[idxi])$statistic,
               Distribution = "Exp-Std")
  )
}
```

```{r ks-plots}
#| echo: false
#| fig-align: center
ggplot(ks_frame, aes(x = ks, fill = Distribution)) +
  geom_density(alpha = 0.4) + theme_minimal() +
  xlab("Kolmogorov-Smirnov Statistic") +
  scale_fill_discrete(
    name = "Learning Method",
    labels = c("Copula-Based", "Copula-Free",
               latex2exp::TeX("$V^*_i \\;|\\; Pa^*_i$"))) +
  theme(legend.position = c(0.7, 0.7),
        legend.background = element_rect(color = "black"),
        legend.text.align = 0.5) +
  ylab("Density")

# ggsave("kolmogorov.png", height = 3, width = 3 * 1.41)
```

As the plot demonstrates, the inference based on Alg. 3 show the smallest distributional distance from the true underlying distribution. However, recourse data learning based on regression $V^*_i \sim \mathrm{Pa}^*_i$ still outperforms the copula-based learning, since the Frank copula model is misspecified for the dataset in question. Thus, we demonstrated that copula-free learning exhibits smaller distributional distance from the ground truth from the remaining options, for cases in which the copula assumptions (i.e., margin stability conditions) are violated.
